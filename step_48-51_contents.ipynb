{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 48 다중 클래스 분류\n",
    "- 이전 단계에서 소프트맥스 함수와 교차 엔트로피 오차를 구했다. \n",
    "- 이번 단계에서는 '스파이럴 데이터셋'이라는 작은 데이터 셋을 사용하여 다중 클래스 분류를 실제로 한다.  \n",
    "\n",
    "  (스파이럴은 나선형 혹은 소용돌이 모양을 뜻)\n",
    "\n",
    "## 48.1 스파이럴 데이터셋\n",
    "- DeZero의 dezero/datasets.py 모듈에는 데이터 셋 관련 클래스와 함수가 준비되어 있다. \n",
    "- 머신러닝에서 사용하는 대표적인 데이터 셋 몇 가지 들어있는데, 여기서는 그중 '스파이럴 데이터셋'을 읽어오겠다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 2)\n",
      "(300,)\n",
      "[-0.12995958 -0.00324155] 1\n",
      "[ 0.3282343  -0.54941994] 0\n"
     ]
    }
   ],
   "source": [
    "import dezero\n",
    "\n",
    "x, t = dezero.datasets.get_spiral(train=True)\n",
    "print(x.shape)\n",
    "print(t.shape)\n",
    "\n",
    "print(x[10], t[10])\n",
    "print(x[110], t[110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get_spiral 함수는 인수로 train이라는 플래그를 받는다.\n",
    "- train=True면 학습(훈련)용 데이터를 반환하고, False면 데스트용 데이터를 반환한다. \n",
    "- 실제로 반환되는 값은 입력 데이터인 x, 정답 데이터(레이블)인 t\n",
    "- x와 t는 모두 ndarray 인스턴스이고 형상은 각각 (300, 2)와 (300, )\n",
    "- 이번-에 다루는 문제는 3클래스 분류이므로 t의 원소는 0, 1, 2중 하나가 된다. \n",
    "- 그림 48-1은 x와 t를 그래프로 표현한 모습이다. \n",
    "\n",
    "\n",
    "<img src=\"./images/48-1.png\" width=\"50%\" height=\"50%\"></img>  \n",
    "\n",
    "- 각각의 클래스를 동그라미, 세모, 엑스 기호로 바꿔 그렸다. \n",
    "- 데이터가 소용돌이 모양으로 분포한다. \n",
    "- 신경망을 사용하여 이 데이터를 정확하게 분류할 수 있는지 살표보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 48.2 학습코드   \n",
    "- 다중 클래스 분류를 하는 코드이고, 전후반으로 나눠서 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import dezero\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "\n",
    "# (1)하이퍼파라미터 설정\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "# (2)데이터 읽기 / 모델, 옵티마이져 생성\n",
    "x, t = dezero.datasets.get_spiral(train=True)\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 지금까지 본 코드와 거의 같다. \n",
    "- (1)에서 우선 하이퍼파라미터를 설정한다. 하이퍼파라미터는 '사람'이 결정하는 매개변수로 은닉층수, 학습률등이 이에 속한다. \n",
    "- (2)에서 데이터셋을 읽고 모델과 옵티마이져 생성\n",
    "- max_epoch = 300으로 설정했는데, 에포크(epoch)는 일종의 단위로 준비된 데이터 셋을 모두 사용했을 때 1에포크이다. \n",
    "- batch_size=30으로 설정하여 데이터를 한번에 30개씩 묶어 처리 하도록 했다. \n",
    "- 은닉층 수는 10 이고, 학습률은 1.0이다. \n",
    "\n",
    "NOTE_   \n",
    "데이터가 많을 때는 모든 데이터를 한꺼번에 처리하는 대신 조금씩 무작위로 모아서 처리한다.  \n",
    "이때의 데이터 뭉치를 미니배치(mini batch)라고 한다.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(x)\n",
    "max_iter = math.ceil(data_size / batch_size) # 소수점 반올림\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    # (3) 데이터셋의 인덱스 뒤섞기 \n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # (4) 미니배치 생성\n",
    "        batch_index = index[i * batch_size:(i + 1) * batch_size]\n",
    "        batch_x = x[batch_index]\n",
    "        batch_t = t[batch_index]\n",
    "\n",
    "        # (5) 기울기 산출 / 매개변수 갱신\n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "\n",
    "    # (6)에포크 마다 학습 경과 출력\n",
    "    avg_loss = sum_loss / data_size\n",
    "    print('epoch %d, loss %.2f' % (epoch + 1, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (3)에서는 np.random.permutation 함수를 사용하여 데이터셋의 인덱스를 무작위로 섞는다.\n",
    "- 이 함수는 인수로 N을 주면 0에서 N-1까지의 정수가 무작위로 배열된 리스트를 반환한다. \n",
    "- 에포크별로 무작위로 정렬된 색인 리스트를 새로 생성하였다.\n",
    "\n",
    "- (4)에서는 미니배치를 생성한다. \n",
    "- batch_index는 방금 생성한 index에서 앞에서부터 차례로 꺼내 사용한다. \n",
    "- batch_x 와 batch_t 모두 ndarray 인스턴스인데 Variable(batch_x)처럼 명시적으로 Variable로 변환해도 제대로 계산된다. (?)\n",
    "\n",
    "- (5)에서는 언제나처럼 기울기를 구하고 매개변수를 갱신한다. \n",
    "\n",
    "- (6)에서는 에포크마다 손실함수의 결과를 출력한다. \n",
    "\n",
    "이상이 스파이럴 데이터셋을 학습하기 위한 코드이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/48-2.png\" width=\"50%\" height=\"50%\"></img>   \n",
    "\n",
    "- 그림 48-2와 같이 학습을 진행할수록 손실이 줄어든다. 올바른 방향으로 학습하고 있다는 뜻.  \n",
    "\n",
    "- 학습이 완료된 신경망은 클래스 영역, 즉 결정경계(decision boundary)를 어떻게 구분하고 있는지 아래와 시각화된다.   \n",
    "\n",
    "<img src=\"./images/48-3.png\" width=\"50%\" height=\"50%\"></img>    \n",
    "\n",
    "- 그림 48-3과 같이 학습 후 신경망은 '소용돌이' 패턴을 제대로 파악하고 있다. \n",
    "- 신경망이 비선형 분리 영역을 학습해낸 것이다.\n",
    "- 신경망에 은닉층을 추가하면 복잡한 표현도 가능해진다.\n",
    "- 층을 더 깊게 쌓는 식으로 표현력을 키울 수 있다는 것이 딥러닝의 특징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 48 코드\n",
    "if '__file__' in globals():\n",
    "    import os, sys\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dezero\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "\n",
    "# Hyperparameters\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "x, t = dezero.datasets.get_spiral(train=True)\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "data_size = len(x)\n",
    "max_iter = math.ceil(data_size / batch_size)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    # Shuffle index for data\n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        batch_index = index[i * batch_size:(i + 1) * batch_size]\n",
    "        batch_x = x[batch_index]\n",
    "        batch_t = t[batch_index]\n",
    "\n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "\n",
    "    # Print loss every epoch\n",
    "    avg_loss = sum_loss / data_size\n",
    "    print('epoch %d, loss %.2f' % (epoch + 1, avg_loss))\n",
    "#--------------------------------------------------------------------------------\n",
    "# Plot boundary area the model predict\n",
    "h = 0.001\n",
    "x_min, x_max = x[:, 0].min() - .1, x[:, 0].max() + .1\n",
    "y_min, y_max = x[:, 1].min() - .1, x[:, 1].max() + .1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "X = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "with dezero.no_grad():\n",
    "    score = model(X)\n",
    "predict_cls = np.argmax(score.data, axis=1)\n",
    "Z = predict_cls.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z)\n",
    "\n",
    "# Plot data points of the dataset\n",
    "N, CLS_NUM = 100, 3\n",
    "markers = ['o', 'x', '^']\n",
    "colors = ['orange', 'blue', 'green']\n",
    "for i in range(len(x)):\n",
    "    c = t[i]\n",
    "    plt.scatter(x[i][0], x[i][1], s=40,  marker=markers[c], c=colors[c])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 49 Dataset 클래스와 전처리\n",
    "\n",
    "- 스파이럴 데이터셋은 300개 정도의 작은 데이터셋이라서 ndarray 인스턴스 하나로 처리할수 있었지만, 대규모 데이터셋을 처리할 때는 문제가 된다. \n",
    "- 거대한 데이터를 하나의 ndarray 인스턴스로 처리하면 모든 원소를 한꺼번에 메모리에 올려하는 문제가 있다.\n",
    "- 그 문제에 대응할 수 있도록 데이터셋 전용 클래스인 Dataset 클래스를 만든다. \n",
    "- Dataset클래스에는 데이터를 전처리할 수 있는 구조도 추가할 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49-1 Dataset 클래스 구현  \n",
    "- Dataset 클래스는 기반 클래스로서의 역할을 하고, \n",
    "- 사용자가 실제로 사용하는 데이터셋은 이를 상속하여 구현하게 할것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, train=True):\n",
    "        self.train = train\n",
    "        self.data = None\n",
    "        self.label = None\n",
    "        self.prepare()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        assert np.isscalar(indx) #index는 정수(스칼라)만 지원\n",
    "        if self.label is None:\n",
    "            return self.data[index], None\n",
    "        else:\n",
    "            return self.data[index], self.label[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def prepare(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 우선 초기화 때 train 인수를 받는다. '학습'과 '테스트'를 구별하기위한 플래그\n",
    "- 인스턴스 변수 data와 label에는 각각 입력데이터와 레이블 보관\n",
    "- 자시클래스에서는 prepare 메서드가 데이터 준비작업을 하도록 구현\n",
    "- 중요한 메서드는 `__getitem__` 과 `__len__` 메서드. 두 메서드가 있어야 'DeZero 데이터셋'이라고 할 수 있다. \n",
    "- `__getitem__` 특수메서드로 괄호를 사용해 접근, 메서드는 단순히 지정된 인덱스에 위치하는 데이터를 꺼낸다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spiral(Dataset):\n",
    "    def prepare(self):\n",
    "        self.data, self.label = get_spiral(self.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- prepare 메서드에서는 인스턴스 변수인 data와 label에 데이터를 설정하는게 다이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.13981389, -0.00721657], dtype=float32), 1)\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "import dezero\n",
    "\n",
    "train_set = dezero.datasets.Spiral(train=True)\n",
    "print(train_set[0])\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0번째 입력 데이터와 레이블이 튜플로 반환되었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49-2 큰 데이터셋의 경우  \n",
    "- 스파이럴 데이터셋 같은 작은 데이터셋이라면 Dataset 클래스의 인스턴스 변수인 data와 label에 직접 ndarray 인스턴스를 유지해도 무리가 없다.\n",
    "- 하지만 데이터셋이 훨씬 크면 위와 같은 구현 방식은 사용할 수 없다. 다음과 같이 생각해보았다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigData(Dataset):\n",
    "    def __getitem__(index):\n",
    "        x = np.load('data/{}.npy'.format(index))\n",
    "        t = np.load('label/{}.npy'.format(index))\n",
    "        return x, t\n",
    "    \n",
    "    def __len__():\n",
    "        return 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49-3 데이터 이어 붙이기 \n",
    "신경망를 학습시킬 때는 데이터셋 중 일부를 미니배치로 꺼낸다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "train_set = dezero.datasets.Spiral()\n",
    "\n",
    "batch_index = [0, 1, 2] # 0에서 2번째까지의 데이터를 꺼냄\n",
    "batch = [train_set[i] for i in batch_index]\n",
    "# batch = [(data_0, label_0), (data_1, label_1),(data_2, label_2)]\n",
    "x = np.array([example[0] for example in batch])\n",
    "t = np.array([example[1] for example in batch])\n",
    "\n",
    "print(x.shape)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49-4 학습코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "train_set = dezero.datasets.Spiral(train=True)\n",
    "model = MLP((hidden_size, 10))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "data_size = len(train_set)\n",
    "max_iter = math.ceil(data_size / batch_size)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    # Shuffle index for data\n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # Create minibatch\n",
    "        batch_index = index[i * batch_size:(i + 1) * batch_size]\n",
    "        batch = [train_set[i] for i in batch_index]\n",
    "        batch_x = np.array([example[0] for example in batch])\n",
    "        batch_t = np.array([example[1] for example in batch])\n",
    "\n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "\n",
    "    # Print loss every epoch\n",
    "    avg_loss = sum_loss / data_size\n",
    "    print('epoch %d, loss %.2f' % (epoch + 1, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49-5 데이터 셋 전처리  \n",
    "- 머신러닝에서는 모델에 데이터를 입력하기 전에 데이터를 특정한 형태로 가공하는 일이 많다. \n",
    "- 전처리(및 데이터 확장)에 대응하기 위해 Dataset 클래스에도 전처리 기능 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, train=True, transform=None, target_transform=None):\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        if self.transform is None:\n",
    "            self.transform = lambda x: x\n",
    "        if self.target_transform is None:\n",
    "            self.target_transform = lambda x: x\n",
    "\n",
    "        self.data = None\n",
    "        self.label = None\n",
    "        self.prepare()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert np.isscalar(index)\n",
    "        if self.label is None:\n",
    "            return self.transform(self.data[index]), None\n",
    "        else:\n",
    "            return self.transform(self.data[index]),\\\n",
    "                   self.target_transform(self.label[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def prepare(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 초기화시에 transform 과 target_transform을 새롭게 받는다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    y = x / 2.0\n",
    "    return y\n",
    "\n",
    "train_set = dezero.datasets.Spiral(transform=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 코드는 입력데이터를 1/2로 스케일 변환하는 전처리 예이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero import transforms\n",
    "\n",
    "f = transforms.Normalize(mean=0.0, std=2.0)\n",
    "train_set = dezero.datasets.Spiral(transform=f) # 책에 오타 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = transforms.Compose([transforms.Normalize(mean=0.0, std=2.0),\n",
    "                       transforms.AsType(np.float64)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 50 미니배치를 뽑아주는 DataLoader \n",
    "- 이전 단계에서는 Dataset 클래스를 만들어서 통일된 인터페이스로 데이터셋을 다룰 수 있게 했다. \n",
    "- 이번 단계에서는 Dataset 클래스에서 미니배치를 뽑아주는 DataLoader 클래스를 구현한다. \n",
    "- DataLoader는 미니배치 생성과 데이터 뒤섞기 등의 기능을 제공하여 사용자가 작성해야 할 학습 코드가 더 간단해진다. \n",
    "- 반복자가 무엇인지 알아보고, DataLoader 클래스를 구현하는 흐름으로 진행하겠다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50.1 반복자란\n",
    "- 이름에서 알 수 있듯이 원소를 반복하여 꺼낸다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [1, 2, 3]\n",
    "x = iter(t)\n",
    "next(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-92de4e9f6b1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 리스트를 반복자로 변환하려면 iter함수를 사용한다. 이 코드는 리스트 t에서 x라는 반복자를 만들었다. \n",
    "- next함수를 사용해 데이터를 순서대로 추출한다. \n",
    "- 네번째 실행에서는 원소가 더 이상 존재하지 않기 때문에 StopIteration 예외가 발생했다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterator:\n",
    "    def __init__(self, max_cnt):\n",
    "        self.max_cnt = max_cnt\n",
    "        self.cnt = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.cnt == self.max_cnt:\n",
    "            raise StopIteration()\n",
    "        \n",
    "        self.cnt += 1\n",
    "        return self.cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MyIterator라는 클래스를 구현했다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "obj = MyIterator(5)\n",
    "for x in obj:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for x in obj: 구문을 사용하여 원소를 꺼낼 수 있다. \n",
    "- 다음은 반복자 구조를 이용하여 미니배치를 뽑아주는 DataLoader 클래스를 구현한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.data_size = len(dataset)\n",
    "        self.max_iter = math.ceil(self.data_size / batch_size)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.iteration = 0 # 반복 횟수 초기화\n",
    "        if self.shuffle:\n",
    "            self.index = np.random.permutation(len(self.dataset)) # 데이터 뒤섞기\n",
    "        else:\n",
    "            self.index = np.arange(len(self.dataset))\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.iteration >= self.max_iter:\n",
    "            self.reset()\n",
    "            raise StopIteration\n",
    "\n",
    "        i, batch_size = self.iteration, self.batch_size\n",
    "        batch_index = self.index[i * batch_size:(i + 1) * batch_size]\n",
    "        batch = [self.dataset[i] for i in batch_index]\n",
    "        x = xp.array([example[0] for example in batch])\n",
    "        t = xp.array([example[1] for example in batch])\n",
    "\n",
    "        self.iteration += 1\n",
    "        return x, t\n",
    "\n",
    "    def next(self):\n",
    "        return self.__next__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 초기화 메서드는 다음 인수를 받는다. \n",
    "    + dataset: Dataset 인터페이스를 만족하는 인스턴스\n",
    "    + batch_size : 배치 크기\n",
    "    + shuffle : 에포크별 데이터셋을 뒤섞을지 여부 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50.2 DataLoader 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2) (10,)\n",
      "(10, 2) (10,)\n"
     ]
    }
   ],
   "source": [
    "from dezero.datasets import Spiral\n",
    "from dezero import DataLoader\n",
    "\n",
    "batch_size = 10\n",
    "max_epoch = 1\n",
    "\n",
    "train_set = Spiral(train=True)\n",
    "test_set = Spiral(train=False)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for x, t in train_loader:\n",
    "        print(x.shape, t.shape) # x, t는 훈련데이터\n",
    "        break\n",
    "        \n",
    "    # 에포크 끝에서 테스트 데이터를 꺼낸다.     \n",
    "    for x, t in test_loader:\n",
    "        print(x.shape, t.shape) # x, t는 테스트 데이터\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 훈련용과 테스트용 각각, 총 두개의 DataLoader를 생성했다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50.3 accuracy 함수 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, t):\n",
    "    y, t = as_variable(y), as_variable(t)\n",
    "    \n",
    "    pred = y.data.argmax(axis=1).reshape(t.shape)\n",
    "    result = (pred == t.data)\n",
    "    acc = result.mean()\n",
    "    return Variable(as_array(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- accuracy 함수는 인수 y와 t를 받아서 '정답률'을 계산해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(0.6666666666666666)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dezero.functions as F\n",
    "\n",
    "y = np.array([[0.2, 0.8, 0], [0.1, 0.9, 0], [0.8, 0.1, 0.1]])\n",
    "t = np.array([1, 2, 0])\n",
    "acc = F.accuracy(y, t)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50.4 스파이럴 데이터셋 학습코드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "train_set = dezero.datasets.Spiral(train=True)\n",
    "test_set = dezero.datasets.Spiral(train=False)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "\n",
    "    for x, t in train_loader: # (1) 훈련용 미니배치 데이터 \n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        acc = F.accuracy(y, t) # (2) 훈련 데이터의 인식 정확도 \n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('epoch: {}'.format(epoch+1))\n",
    "    print('train loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(train_set), sum_acc / len(train_set)))\n",
    "\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    with dezero.no_grad(): # 기울기 불필요 모드\n",
    "        for x, t in test_loader: # 테스트용 미니배치 데이터\n",
    "            y = model(x)\n",
    "            loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t) # 테스트 데이터의 인식 정확동 \n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('test loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(test_set), sum_acc / len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/50-1.png\" ></img>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그림50-1을 보면...\n",
    "\n",
    "NOTE_\n",
    "과대적합(overfiting)은 특정 훈련 데이터에 지나치게 최적화된 상태를 말한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 51 MNIST 학습  \n",
    "\n",
    "- 지금까지 데이터셋을 쉽게 다룰 수 있는 구조를 마련했다. \n",
    "- Dataset 클래스로 데이터셋 처리를 위한 공통 인터페이스를 마련했고,\n",
    "- '전처리'를 설정할 수 있도록 했다. \n",
    "- DataLoader 클래스로는 Dataset에서 미니배치 단위로 데이터를 꺼내올 수 있게 했다.   \n",
    "<img src=\"./images/51-1.png\" ></img>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 51.1 MNIST 데이터 셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'dezero' has no attribute 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e40dc72fd109>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdezero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdezero\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdezero\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'dezero' has no attribute 'datasets'"
     ]
    }
   ],
   "source": [
    "import dezero\n",
    "\n",
    "train_set = dezero.datasets.MNIST(train=True, transform=None)\n",
    "test_set = dezero.datasets.MNIST(train=True, transform=None)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t = train_set[0]\n",
    "print(type(x), x.shape)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터 예시\n",
    "x, t = train_set[0] # 0번째 (data, label) 추출\n",
    "plt.show(x.reshape(28,28), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print('label:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/51-2.png\" width=\"50%\" height=\"50%\"></img>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x = x.flatten()\n",
    "    x = x.astype(np.float32)\n",
    "    x /= 255.0\n",
    "    return x\n",
    "\n",
    "train_set = dezero.datasets.MNIST(train=True, transform=f)\n",
    "test_set = dezero.datasets.MNIST(train=False, transform=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 51.2 MNIST 데이터 셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 5\n",
    "batch_size = 100\n",
    "hidden_size = 1000\n",
    "\n",
    "train_set = dezero.datasets.MNIST(train=True)\n",
    "test_set = dezero.datasets.MNIST(train=False)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "model = MLP((hidden_size, 10))\n",
    "optimizer = optimizers.SGD().setup(model)\n",
    "#model = MLP((hidden_size, hidden_size, 10), activation=F.relu)\n",
    "#optimizer = optimizers.Adam().setup(model)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "\n",
    "    for x, t in train_loader:\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        acc = F.accuracy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('epoch: {}'.format(epoch+1))\n",
    "    print('train loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(train_set), sum_acc / len(train_set)))\n",
    "\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    with dezero.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            y = model(x)\n",
    "            loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t)\n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('test loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(test_set), sum_acc / len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 51.3 모델 개선\n",
    "\n",
    "ReLU는 입력이 0보다 크면 입력 그대로 출력하고, 0 이하면 0을 출력하는 함수이다. \n",
    "$$h(x)=\\begin{cases}\n",
    "x \\text{ }(x>0) \\\\\n",
    "0 \\text{ }(x\\leq0)\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Function):\n",
    "    def forward(self, x):\n",
    "        xp = cuda.get_array_module(x)\n",
    "        y = xp.maximum(x, 0.0) # (1)\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        x, = self.inputs\n",
    "        mask = x.data > 0 #(2)\n",
    "        gx = gy * mask #(3)\n",
    "        return gx\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return ReLU()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = MLP((hidden_size, 10))\n",
    "model = MLP((hidden_size, hidden_size, 10), activation=F.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3층 신경망을 만들었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 칼럼 : 딥러닝 프레임워크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define-by-Run 방식의 자동미분\n",
    "- 계층 컬렉션\n",
    "- 옵티마이져 컬렉션"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
