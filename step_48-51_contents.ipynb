{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 48 다중 클래스 분류\n",
    "- 이전 단계에서 소프트맥스 함수와 교차 엔트로피 오차를 구했다. \n",
    "- 이번 단계에서는 '스파이럴 데이터셋'이라는 작은 데이터 셋을 사용하여 다중 클래스 분류를 실제로 한다.  \n",
    "\n",
    "  (스파이럴은 나선형 혹은 소용돌이 모양을 뜻)\n",
    "\n",
    "## 48.1 스파이럴 데이터셋\n",
    "- DeZero의 dezero/datasets.py 모듈에는 데이터 셋 관련 클래스와 함수가 준비되어 있다. \n",
    "- 머신러닝에서 사용하는 대표적인 데이터 셋 몇 가지 들어있는데, 여기서는 그중 '스파이럴 데이터셋'을 읽어오겠다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 2)\n",
      "(300,)\n",
      "[-0.12995958 -0.00324155] 1\n",
      "[ 0.3282343  -0.54941994] 0\n"
     ]
    }
   ],
   "source": [
    "import dezero\n",
    "\n",
    "x, t = dezero.datasets.get_spiral(train=True)\n",
    "print(x.shape)\n",
    "print(t.shape)\n",
    "\n",
    "print(x[10], t[10])\n",
    "print(x[110], t[110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get_spiral 함수는 인수로 train이라는 플래그를 받는다.\n",
    "- train=True면 학습(훈련)용 데이터를 반환하고, False면 데스트용 데이터를 반환한다. \n",
    "- 실제로 반환되는 값은 입력 데이터인 x, 정답 데이터(레이블)인 t\n",
    "- x와 t는 모두 ndarray 인스턴스이고 형상은 각각 (300, 2)와 (300, )\n",
    "- 이번-에 다루는 문제는 3클래스 분류이므로 t의 원소는 0, 1, 2중 하나가 된다. \n",
    "- 그림 48-1은 x와 t를 그래프로 표현한 모습이다. \n",
    "\n",
    "\n",
    "<img src=\"./images/48-1.png\" width=\"50%\" height=\"50%\"></img>  \n",
    "\n",
    "- 각각의 클래스를 동그라미, 세모, 엑스 기호로 바꿔 그렸다. \n",
    "- 데이터가 소용돌이 모양으로 분포한다. \n",
    "- 신경망을 사용하여 이 데이터를 정확하게 분류할 수 있는지 살표보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 48.2 학습코드   \n",
    "- 다중 클래스 분류를 하는 코드이고, 전후반으로 나눠서 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import dezero\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "\n",
    "# (1)하이퍼파라미터 설정\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "# (2)데이터 읽기 / 모델, 옵티마이져 생성\n",
    "x, t = dezero.datasets.get_spiral(train=True)\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 지금까지 본 코드와 거의 같다. \n",
    "- (1)에서 우선 하이퍼파라미터를 설정한다. 하이퍼파라미터는 '사람'이 결정하는 매개변수로 은닉층수, 학습률등이 이에 속한다. \n",
    "- (2)에서 데이터셋을 읽고 모델과 옵티마이져 생성\n",
    "- max_epoch = 300으로 설정했는데, 에포크(epoch)는 일종의 단위로 준비된 데이터 셋을 모두 사용했을 때 1에포크이다. \n",
    "- batch_size=30으로 설정하여 데이터를 한번에 30개씩 묶어 처리 하도록 했다. \n",
    "- 은닉층 수는 10 이고, 학습률은 1.0이다. \n",
    "\n",
    "NOTE_   \n",
    "데이터가 많을 때는 모든 데이터를 한꺼번에 처리하는 대신 조금씩 무작위로 모아서 처리한다.  \n",
    "이때의 데이터 뭉치를 미니배치(mini batch)라고 한다.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(x)\n",
    "max_iter = math.ceil(data_size / batch_size) # 소수점 반올림\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    # (3) 데이터셋의 인덱스 뒤섞기 \n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # (4) 미니배치 생성\n",
    "        batch_index = index[i * batch_size:(i + 1) * batch_size]\n",
    "        batch_x = x[batch_index]\n",
    "        batch_t = t[batch_index]\n",
    "\n",
    "        # (5) 기울기 산출 / 매개변수 갱신\n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "\n",
    "    # (6)에포크 마다 학습 경과 출력\n",
    "    avg_loss = sum_loss / data_size\n",
    "    print('epoch %d, loss %.2f' % (epoch + 1, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (3)에서는 np.random.permutation 함수를 사용하여 데이터셋의 인덱스를 무작위로 섞는다.\n",
    "- 이 함수는 인수로 N을 주면 0에서 N-1까지의 정수가 무작위로 배열된 리스트를 반환한다. \n",
    "- 에포크별로 무작위로 정렬된 색인 리스트를 새로 생성하였다.\n",
    "\n",
    "- (4)에서는 미니배치를 생성한다. \n",
    "- batch_index는 방금 생성한 index에서 앞에서부터 차례로 꺼내 사용한다. \n",
    "- batch_x 와 batch_t 모두 ndarray 인스턴스인데 Variable(batch_x)처럼 명시적으로 Variable로 변환해도 제대로 계산된다. (?)\n",
    "\n",
    "- (5)에서는 언제나처럼 기울기를 구하고 매개변수를 갱신한다. \n",
    "\n",
    "- (6)에서는 에포크마다 손실함수의 결과를 출력한다. \n",
    "\n",
    "이상이 스파이럴 데이터셋을 학습하기 위한 코드이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/48-2.png\" width=\"50%\" height=\"50%\"></img>   \n",
    "\n",
    "- 그림 48-2와 같이 학습을 진행할수록 손실이 줄어든다. 올바른 방향으로 학습하고 있다는 뜻.  \n",
    "\n",
    "- 학습이 완료된 신경망은 클래스 영역, 즉 결정경계(decision boundary)를 어떻게 구분하고 있는지 아래와 시각화된다.   \n",
    "\n",
    "<img src=\"./images/48-3.png\" width=\"50%\" height=\"50%\"></img>    \n",
    "\n",
    "- 그림 48-3과 같이 학습 후 신경망은 '소용돌이' 패턴을 제대로 파악하고 있다. \n",
    "- 신경망이 비선형 분리 영역을 학습해낸 것이다.\n",
    "- 신경망에 은닉층을 추가하면 복잡한 표현도 가능해진다.\n",
    "- 층을 더 깊게 쌓는 식으로 표현력을 키울 수 있다는 것이 딥러닝의 특징이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 48 코드\n",
    "if '__file__' in globals():\n",
    "    import os, sys\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dezero\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "from dezero.models import MLP\n",
    "\n",
    "# Hyperparameters\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "x, t = dezero.datasets.get_spiral(train=True)\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "data_size = len(x)\n",
    "max_iter = math.ceil(data_size / batch_size)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    # Shuffle index for data\n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        batch_index = index[i * batch_size:(i + 1) * batch_size]\n",
    "        batch_x = x[batch_index]\n",
    "        batch_t = t[batch_index]\n",
    "\n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "\n",
    "    # Print loss every epoch\n",
    "    avg_loss = sum_loss / data_size\n",
    "    print('epoch %d, loss %.2f' % (epoch + 1, avg_loss))\n",
    "#--------------------------------------------------------------------------------\n",
    "# Plot boundary area the model predict\n",
    "h = 0.001\n",
    "x_min, x_max = x[:, 0].min() - .1, x[:, 0].max() + .1\n",
    "y_min, y_max = x[:, 1].min() - .1, x[:, 1].max() + .1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "X = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "with dezero.no_grad():\n",
    "    score = model(X)\n",
    "predict_cls = np.argmax(score.data, axis=1)\n",
    "Z = predict_cls.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z)\n",
    "\n",
    "# Plot data points of the dataset\n",
    "N, CLS_NUM = 100, 3\n",
    "markers = ['o', 'x', '^']\n",
    "colors = ['orange', 'blue', 'green']\n",
    "for i in range(len(x)):\n",
    "    c = t[i]\n",
    "    plt.scatter(x[i][0], x[i][1], s=40,  marker=markers[c], c=colors[c])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 49 Dataset 클래스와 전처리\n",
    "\n",
    "- 스파이럴 데이터셋은 300개 정도의 작은 데이터셋이라서 ndarray 인스턴스 하나로 처리할수 있었지만, 대규모 데이터셋을 처리할 때는 문제가 된다. \n",
    "- 거대한 데이터를 하나의 ndarray 인스턴스로 처리하면 모든 원소를 한꺼번에 메모리에 올려하는 문제가 있다.\n",
    "- 그 문제에 대응할 수 있도록 데이터셋 전용 클래스인 Dataset 클래스를 만든다. \n",
    "- Dataset클래스에는 데이터를 전처리할 수 있는 구조도 추가할 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49-1 Dataset 클래스 구현  \n",
    "- Dataset 클래스는 기반 클래스로서의 역할을 하고, \n",
    "- 사용자가 실제로 사용하는 데이터셋은 이를 상속하여 구현하게 할것이다. \n",
    "- Dataset 클래스를 구현해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, train=True):\n",
    "        self.train = train\n",
    "        self.data = None\n",
    "        self.label = None\n",
    "        self.prepare()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        assert np.isscalar(indx) #index는 정수(스칼라)만 지원\n",
    "        if self.label is None:\n",
    "            return self.data[index], None\n",
    "        else:\n",
    "            return self.data[index], self.label[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def prepare(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 우선 초기화 때 train 인수를 받는다. 이 인수는 '학습'과 '테스트'를 구별하기위한 플래그이다. \n",
    "- 인스턴스 변수 data와 label에는 각각 입력데이터와 레이블 보관한다.\n",
    "- 자식클래스에서는 prepare 메서드가 데이터 준비작업을 하도록 구현해야 한다.\n",
    "- 중요한 메서드는 `__getitem__` 과 `__len__` 메서드이다. 두 메서드가 있어야 'DeZero 데이터셋'이라고 할 수 있다. \n",
    "- 이렇게 인터페이스를 통일하면 다양한 데이터셋을 교체해가며 사용할 수가 있다. \n",
    "- `__getitem__` 특수메서드로 괄호를 사용해 접근할 때의 동작을 정의하고, 메서드는 단순히 지정된 인덱스에 위치하는 데이터를 꺼낸다.  \n",
    "- 레이블 데이터가 없다면 입력 데이터 self.data[index]의 레이블은 None을 반환한다.\n",
    "- 데이터셋의 길이를 알려주는 `_len_`메서드는 len 함수를 사용할 때 호출된다.   \n",
    "\n",
    "`CAUTION_ `   \n",
    "`_getitem_`메서드는 '슬라이스'조작도 처리할 수 있다. 그러나 DeZero의 Dataset 클래스는 슬라이스에는 대응하지 않고, 단순히 정수를 index로 사용하는 조작법만 지원하게 했다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spiral(Dataset):\n",
    "    def prepare(self):\n",
    "        self.data, self.label = get_spiral(self.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset 클래스를 상속하여 스파이럴 데이터셋을 구현한 것이다. \n",
    "- prepare 메서드에서는 인스턴스 변수인 data와 label에 데이터를 설정하는게 다이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.13981389, -0.00721657], dtype=float32), 1)\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "import dezero\n",
    "\n",
    "train_set = dezero.datasets.Spiral(train=True)\n",
    "print(train_set[0])\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spiral 클래스를 사용하여 데이터를 추출 할 수 있다. \n",
    "- 괄호를 사용하여 train_set[0] 형태로 데이터를 가져왔고, 0번째 입력 데이터와 레이블이 튜플로 반환되었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49-2 큰 데이터셋의 경우  \n",
    "- 스파이럴 데이터셋 같은 작은 데이터셋이라면 Dataset 클래스의 인스턴스 변수인 data와 label에 직접 ndarray 인스턴스를 유지해도 무리가 없다.\n",
    "- 하지만 데이터셋이 훨씬 크면 위와 같은 구현 방식은 사용할 수 없다.   \n",
    "- 다음과 같이 생각해보았다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigData(Dataset):\n",
    "    def __getitem__(index):\n",
    "        x = np.load('data/{}.npy'.format(index))\n",
    "        t = np.load('label/{}.npy'.format(index))\n",
    "        return x, t\n",
    "    \n",
    "    def __len__():\n",
    "        return 1000000 #백만"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data 디렉토리와 label 디렉토리에 각각 100만 개의 데이터(레이블)가 저장되어 있다고 가정한다. \n",
    "- BigData 클래스를 초기화할 때는 데이터를 아직 읽지 않고, 데이터에 접근할 때 비로소 읽게 하는 것이다.\n",
    "- 구체적으로 `__getitem__(index)` 가 불리는 시점에서 data 디렉토리에 있는 데이터를 읽는다. (np.load 함수는 53단계에서 설명)  \n",
    "- 'DeZero 데이터셋'이 되기 위한 요건은 `__getitem__`과 `__len__` 두 메서드를 구현하는 것이다.   \n",
    "- 앞의 BiData 클래스는 요건을 충족하므로 Spiral 클래스처럼 사용할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49-3 데이터 이어 붙이기 \n",
    "- 이어서 Spiral 클래스를 사용하여 학습코드를 작성해보자. \n",
    "- 신경망를 학습시킬 때는 데이터셋 중 일부를 미니배치로 꺼낸다. \n",
    "- 다음음 Spiral 클래스를 사용하여 데이터를 미니 배치로 가져오는 코드이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "train_set = dezero.datasets.Spiral()\n",
    "\n",
    "batch_index = [0, 1, 2] # 0에서 2번째까지의 데이터를 꺼냄\n",
    "batch = [train_set[i] for i in batch_index]\n",
    "# batch = [(data_0, label_0), (data_1, label_1),(data_2, label_2)]\n",
    "\n",
    "x = np.array([example[0] for example in batch])\n",
    "t = np.array([example[1] for example in batch])\n",
    "\n",
    "print(x.shape)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 우선은 인덱스를 지정하여 여러 데이터(미니배치)를 꺼낸다. \n",
    "- 이 코드를 실행하면 batch에 여러 데이터가 리스트로 저장된다. \n",
    "- 이 데이터를 DeZero의 신경망에 입력하려면 하나의 ndarray 인스턴스로 변환해야 한다. \n",
    "- batch의 각 원소에서 데이터(혹은 레이블)만을 꺼내 하나의 ndarray 인스턴스로 변형했다.(이어 붙였다.)\n",
    "- 드디어 신경망에 입력할 수 있는 형태가 마련되었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49-4 학습코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spiral 클래스를 사용하여 학습을 해보겠다. \n",
    "- 파이썬 임포트 코드는 생략했고, 달라진 부분은 네군데이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "train_set = dezero.datasets.Spiral(train=True) # 달라진 부분\n",
    "model = MLP((hidden_size, 10))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "data_size = len(train_set)\n",
    "max_iter = math.ceil(data_size / batch_size)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    # Shuffle index for data\n",
    "    index = np.random.permutation(data_size)\n",
    "    sum_loss = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # Create minibatch\n",
    "        batch_index = index[i * batch_size:(i + 1) * batch_size]\n",
    "        batch = [train_set[i] for i in batch_index] # 달라진 부분\n",
    "        batch_x = np.array([example[0] for example in batch]) # 달라진 부분 \n",
    "        batch_t = np.array([example[1] for example in batch]) # 달라진 부분 \n",
    "\n",
    "        y = model(batch_x)\n",
    "        loss = F.softmax_cross_entropy(y, batch_t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(batch_t)\n",
    "\n",
    "    # Print loss every epoch\n",
    "    avg_loss = sum_loss / data_size\n",
    "    print('epoch %d, loss %.2f' % (epoch + 1, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이전 단계에서 달라진 부분은 Spiral 클래스를 사용하는 내용이다. \n",
    "- 그에 따라 미니배치를 만드는 부분의 코드를 수정했다.\n",
    "- 그 외에는 이전단계와 같고, 실행하면 이전 단계와 마찬가지로 손실(loss)이 낮아지는 모습을 확인할 수 있다. \n",
    "- Dataset 클래스를 사용하여 신경망을 학습할 수 있게 되었다. \n",
    "- Dataset 클래스를 사용하는 이점은 인터페이스를 통일하여 다양한 데이터셋을 똑같은 코드로 처리할 수 있는 것이다. \n",
    "- Spiral을 BigData로 교체하는 것만으로 훨씬 큰 데이터셋에 대응할 수 있다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49-5 데이터 셋 전처리  \n",
    "- 머신러닝에서는 모델에 데이터를 입력하기 전에 데이터를 특정한 형태로 가공하는 일이 많다. \n",
    "- 예를 들어 데이터에서 특정 값을 제거하거나 데이터의 형상을 변형하는 처리이다. \n",
    "- 이미지를 회전 혹은 좌우 반전시키거나 데이터 수를 인위적으로 늘리는 기술(데이터 확장)도 자주 사용된다. \n",
    "- 전처리(및 데이터 확장)에 대응하기 위해 Dataset 클래스에도 전처리 기능을 추가하겠다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, train=True, transform=None, target_transform=None):\n",
    "        self.train = train\n",
    "        self.transform = transform # <---\n",
    "        self.target_transform = target_transform # <---\n",
    "        if self.transform is None: # <---\n",
    "            self.transform = lambda x: x # <---\n",
    "        if self.target_transform is None: # <---\n",
    "            self.target_transform = lambda x: x # <---\n",
    "\n",
    "        self.data = None\n",
    "        self.label = None\n",
    "        self.prepare()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert np.isscalar(index)\n",
    "        if self.label is None:\n",
    "            return self.transform(self.data[index]), None # <---\n",
    "        else:\n",
    "            return self.transform(self.data[index]),\\  # <---\n",
    "                   self.target_transform(self.label[index]) # <---\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def prepare(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 초기화시에 transform 과 target_transform을 새롭게 받는다.   \n",
    "- 이 인수들은 호출 가능한 객체를 받는다. (예를 들어 '파이썬 함수')\n",
    "- trainsform은 입력 데이터 하나에 대한 변환을 처리하고, target_transform 레이블 하나에 대한 변환을 처리한다. \n",
    "- 이 값이 None이라면 전처리 로직은 lambda x:x로 설정되고, 받은 인수는 그대로 반환한다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    y = x / 2.0\n",
    "    return y\n",
    "\n",
    "train_set = dezero.datasets.Spiral(transform=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 코드는 입력데이터를 1/2로 스케일 변환하는 전처리 예이다.  \n",
    "- 참고로 Dezero는 데이터 정규화와 이미지 데이터 변환 등 전처리 시 자주 사용되는 변환들을 transforms.py에 뒀다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dezero import transforms\n",
    "\n",
    "f = transforms.Normalize(mean=0.0, std=2.0)\n",
    "train_set = dezero.datasets.Spiral(transform=f) # 책에 오타 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f 에 의해 입력을 x라 할 때 (x - mean) / std 로 변환이 이루어 진다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = transforms.Compose([transforms.Normalize(mean=0.0, std=2.0),\n",
    "                       transforms.AsType(np.float64)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 변환 처리를 연달아 수행할 수도 있다.\n",
    "- transform.Compose 클래스는 주어진 변환 목록을 앞에서 순서대로 처리한다. \n",
    "- 예에서는 정규화를 먼저 하고, 연이어 데이터 타입을 np.float64로 변환하는 일을 수행한다. \n",
    "- transform.py에는 편리한 변환 처리가 준비되어 있다. 참고하라. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 50 미니배치를 뽑아주는 DataLoader \n",
    "- 이전 단계에서는 Dataset 클래스를 만들어서 통일된 인터페이스로 데이터셋을 다룰 수 있게 했다. \n",
    "- 이번 단계에서는 Dataset 클래스에서 미니배치를 뽑아주는 DataLoader 클래스를 구현한다. \n",
    "- DataLoader는 미니배치 생성과 데이터 뒤섞기 등의 기능을 제공하여 사용자가 작성해야 할 학습 코드가 더 간단해진다. \n",
    "- 반복자가 무엇인지 알아보고, DataLoader 클래스를 구현하는 흐름으로 진행하겠다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50.1 반복자란\n",
    "- 이름에서 알 수 있듯이 원소를 반복하여 꺼낸다. \n",
    "- 파이썬의 반복자는 리스트나 튜플 등 여러 원소를 담고 있는 데이터 타입으로부터 데이터를 순차적으로 추출하는 기능을 제공한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [1, 2, 3]\n",
    "x = iter(t)\n",
    "next(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-92de4e9f6b1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 리스트를 반복자로 변환하려면 iter함수를 사용한다. 이 코드는 리스트 t에서 x라는 반복자를 만들었다. \n",
    "- next함수를 사용해 데이터를 순서대로 추출한다. \n",
    "- 네번째 실행에서는 원소가 더 이상 존재하지 않기 때문에 StopIteration 예외가 발생했다. \n",
    "\n",
    "NOTE_  \n",
    "for 문에서 리스트의 원소를 꺼낼 때 내부적으로 반복자가 이용된다.   \n",
    "예를 들어 t = [1, 2, 3]일 때 for x in t: x를 실행하면 리스트 t가 내부적으로 반복자로  변환된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterator:\n",
    "    def __init__(self, max_cnt):\n",
    "        self.max_cnt = max_cnt\n",
    "        self.cnt = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.cnt == self.max_cnt:\n",
    "            raise StopIteration()\n",
    "        \n",
    "        self.cnt += 1\n",
    "        return self.cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 코드는 고유한 반복자를 만들어 준다. \n",
    "- MyIterator라는 클래스를 구현했다.\n",
    "- 클래스를 파이썬 반복자로 사용하려면 `__iter__`특수 메서드를 구현하고 자기 자신(self)을 반환하도록 한다. \n",
    "- `__next__` 특수 메서드에서는 다음 원소를 반환하도록 구현한다. \n",
    "- 만약 반환할 원소가 없다면 raise StopIteration()을 수행한다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "obj = MyIterator(5)\n",
    "for x in obj:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for x in obj: 구문을 사용하여 원소를 꺼낼 수 있다. \n",
    "- 다음은 반복자 구조를 이용하여 미니배치를 뽑아주는 DataLoader 클래스를 구현한다. \n",
    "- 기본적으로 DataLoader는 주어진 데이터셋의 첫 데이터부터 차례로 꺼내주지만, 필요에 따라 뒤섞을 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.data_size = len(dataset)\n",
    "        self.max_iter = math.ceil(self.data_size / batch_size)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.iteration = 0 # 반복 횟수 초기화\n",
    "        if self.shuffle:\n",
    "            self.index = np.random.permutation(len(self.dataset)) # 데이터 뒤섞기\n",
    "        else:\n",
    "            self.index = np.arange(len(self.dataset))\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.iteration >= self.max_iter:\n",
    "            self.reset()\n",
    "            raise StopIteration\n",
    "\n",
    "        i, batch_size = self.iteration, self.batch_size\n",
    "        batch_index = self.index[i * batch_size:(i + 1) * batch_size]\n",
    "        batch = [self.dataset[i] for i in batch_index]\n",
    "        x = xp.array([example[0] for example in batch])\n",
    "        t = xp.array([example[1] for example in batch])\n",
    "\n",
    "        self.iteration += 1\n",
    "        return x, t\n",
    "\n",
    "    def next(self):\n",
    "        return self.__next__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 초기화 메서드는 다음 인수를 받는다. \n",
    "    + dataset: Dataset 인터페이스를 만족하는 인스턴스\n",
    "    + batch_size : 배치 크기\n",
    "    + shuffle : 에포크별 데이터셋을 뒤섞을지 여부 \n",
    "- 초기화 코드는 인수를 인스턴스 변수로 저장한 후 reset 메서드를 부른다. \n",
    "- reset메서드에서는 인스턴스 변수의 반복 횟수를 0으로 설정하고 필요에 따라 데이터이 인덱스를 뒤섞는다. \n",
    "- `__next__` 메서드는 미니배치를 꺼내 ndarray 인스턴스로 변환한다. \n",
    "- `dezero/__init__.py` 에 from dezero.dataloaders import DataLoader라는 임포트문을 추가하면 사용자는 DataLoader를 임포트할 수 있다. \n",
    "\n",
    "CAUTION_   \n",
    "dezero/dataloaders.py의 DataLoader클래스는 데이터를 GPU로 전송하는 로직도 포함하지만 지면에서 생략한다. \n",
    "GPU는 52단계에서 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50.2 DataLoader 사용하기  \n",
    "- DataLoader 클래스를 사용하면 미니배치를 꺼내오는 일이 간단해진다. \n",
    "- 시험삼아 신경망 학습을 가정하고 DataLoader를 사용해보겠다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2) (10,)\n",
      "(10, 2) (10,)\n"
     ]
    }
   ],
   "source": [
    "from dezero.datasets import Spiral\n",
    "from dezero import DataLoader\n",
    "\n",
    "batch_size = 10\n",
    "max_epoch = 1\n",
    "\n",
    "train_set = Spiral(train=True)\n",
    "test_set = Spiral(train=False)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for x, t in train_loader:\n",
    "        print(x.shape, t.shape) # x, t는 훈련데이터\n",
    "        break\n",
    "        \n",
    "    # 에포크 끝에서 테스트 데이터를 꺼낸다.     \n",
    "    for x, t in test_loader:\n",
    "        print(x.shape, t.shape) # x, t는 테스트 데이터\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 훈련용과 테스트용 각각, 총 두개의 DataLoader를 생성했다. \n",
    "- 훈련용 DataLoader는 에포크별로 데이터를 뒤섞어야 하기 때문에 shuffle=True(기본값) 으로 설정한다. \n",
    "- 테스트용 DataLoader는 정확도 평가에만 사용하므로 shuffle=False로 설정한다. \n",
    "- 이렇게 해주면 이후 미니배치 추출과 데이터 뒤섞기는 DataLoader가 알아서 해준다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50.3 accuracy 함수 구현하기\n",
    "- DataLoader를 사용해보기 전에 편의 기능을 하나 추가하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, t):\n",
    "    y, t = as_variable(y), as_variable(t)\n",
    "    \n",
    "    pred = y.data.argmax(axis=1).reshape(t.shape)\n",
    "    result = (pred == t.data)\n",
    "    acc = result.mean()\n",
    "    return Variable(as_array(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- accuracy 함수는 인수 y와 t를 받아서 '정답률'을 계산해준다. \n",
    "- y는 신경망의 예측 결과이고, t는 정답 데이터이다. 이 두 인수는 Variable 또는 ndarray 인스턴스여야 한다. \n",
    "- 내용을 보면 먼저 신경망의 예측 결과를 구해 pred에 저장한다. 예측 결과의 최대 인덱스를 찾아서 형상을 변경해야 한다. \n",
    "- pred와 정답 데이터 t를 비교하면 결과는 True / False의 텐서(ndarray)가 된다. \n",
    "- 이 텐서의 True 비율(평균)이 정답률에 해당한다. \n",
    "- 코드 마지막줄에 as_array 함수를 사용하고 있다. \n",
    "- result.mean()이 반환하는 데이터의 형식이 np.float64나 np.float32이기 때문에 as_array함수를 이용하여 ndarray 인스턴스로 변환한 것이다. \n",
    "\n",
    "CAUTION_  \n",
    "accuracy 함수는 Variable 인스턴스를 반환합니다만 내부 계산은 ndarray 인스턴스를 사용해 수행한다. \n",
    "따라서 accuracy 함수로 수행한 계산은 미분할 수 없다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable(0.6666666666666666)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dezero.functions as F\n",
    "\n",
    "y = np.array([[0.2, 0.8, 0], [0.1, 0.9, 0], [0.8, 0.1, 0.1]])\n",
    "t = np.array([1, 2, 0])\n",
    "acc = F.accuracy(y, t)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- y에 담긴 3개의 샘플 데이터에 대한 신경망이 어떻게 예측하는지 보았다. (3-클래스 분류)\n",
    "- 정답 데이터인 t에는 각 샘플 데이터의 정답 인덱스가 부여되어 있다. \n",
    "- 결과는 0.6666... 정확하게 나왔다.(0번과 2번 샘플은 정답을 맞혔고, 1번 샘플에 대해서는 틀렸으니 정확도는 2/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50.4 스파이럴 데이터셋 학습코드  \n",
    "DataLoader 클래스와 accuracy 함수를 사용하여 스파이럴 데이터셋을 학습해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "lr = 1.0\n",
    "\n",
    "train_set = dezero.datasets.Spiral(train=True)\n",
    "test_set = dezero.datasets.Spiral(train=False)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "\n",
    "    for x, t in train_loader: # (1) 훈련용 미니배치 데이터 \n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        acc = F.accuracy(y, t) # (2) 훈련 데이터의 인식 정확도 \n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('epoch: {}'.format(epoch+1))\n",
    "    print('train loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(train_set), sum_acc / len(train_set)))\n",
    "\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    with dezero.no_grad(): # (3)기울기 불필요 모드\n",
    "        for x, t in test_loader: # (4)테스트용 미니배치 데이터\n",
    "            y = model(x)\n",
    "            loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t) # (5)테스트 데이터의 인식 정확동 \n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('test loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(test_set), sum_acc / len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (1)에서는 DataLoader를 사용해 미니배치를 꺼내고, \n",
    "- (2)에서 accuracy 함수를 사용하여 인식 정확도를 계산한다.\n",
    "- (3)에서는 에포크별로 테스트 데이터셋을 사용하여 훈련결과를 평가한다\n",
    "- 테스트 시에는 역전파가 필요 없으므로 with dezero.no_grad(): 블록 내부로 들어간다. 이렇게 함으로써 역전파 관련 처리와 자원소모를 피할 수 있다.\n",
    "- (4)에서는 테스트용 DataLoader에서 미니배치 데이터를 꺼내 평가한다. \n",
    "- (5)에서는 마지막으로 accuracy 함수를 사용하여 인식 정확도를 계산한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/50-1.png\" ></img>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그림50-1을 보면 에포크가 진행됨에 따라 손실(loss)이 낮아지고 인식 정확도(accuracy)는 상승하고 있다. \n",
    "- 학습이 제대로 이루어지고 있다는 증거이다.\n",
    "- 또한, 훈련(train)과 테스트(test)의 차이가 작은데, 모델이 과대적합을 일으키지 않았다는 뜻이다. \n",
    "\n",
    "NOTE_  \n",
    "과대적합(overfiting)은 특정 훈련 데이터에 지나치게 최적화된 상태를 말한다.    \n",
    "새로운 데이터에서는 예측 정확도가 훨씬 떨어지는, 일반화되지 못한 상태를 뜻한다.   \n",
    "신경망으로는 표현력이 높은 모델을 만들 수 있기 때문에 과대적합이 흔하게 일어난다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 51 MNIST 학습  \n",
    "\n",
    "- 지금까지 데이터셋을 쉽게 다룰 수 있는 구조를 마련했다. \n",
    "- Dataset 클래스로 데이터셋 처리를 위한 공통 인터페이스를 마련했고,\n",
    "- '전처리'를 설정할 수 있도록 했다. \n",
    "- DataLoader 클래스로는 Dataset에서 미니배치 단위로 데이터를 꺼내올 수 있게 했다.   \n",
    "<img src=\"./images/51-1.png\" ></img>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전처리를 수행하는 객체를 호출 가능하다는 뜻에서 Callable로 표시했다. \n",
    "- Callable은 Dataset이 보유하고 Dataset은 DataLoader가 보유하는 관계이다. \n",
    "- 그리고 사용자는 DataLoader로부터 미니배치를 가져온다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 51.1 MNIST 데이터 셋 \n",
    "- 우선 MNIST 데이터셋이 무엇인지부터 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: train-images-idx3-ubyte.gz\n",
      "[##############################] 100.00% Done\n",
      "Downloading: train-labels-idx1-ubyte.gz\n",
      "[##############################] 100.00% Done\n",
      "60000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "import dezero\n",
    "\n",
    "train_set = dezero.datasets.MNIST(train=True, transform=None)\n",
    "test_set = dezero.datasets.MNIST(train=True, transform=None)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 훈련 데이터와 테스트 데이터를 가져옵니다. \n",
    "- transform = None으로 설정하여 아무런 전처리도 수행하지 않도록 했다. \n",
    "- 훈련 데이터(train_set)와 테스트 데이터(test_set)의 길이를 확인해보니 각각 60000과 10000이 나왔다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (1, 28, 28)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "x, t = train_set[0]\n",
    "print(type(x), x.shape)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train_set 의 0번째 샘플 데이터를 확인해보는 코드이다. \n",
    "- MNIST 데이터셋도 (data, label) 튜플 형태로 구성되어 있다. data에는 0~9까지 손글씨 숫자 이미지가 들어있다. \n",
    "- data에 1채널(그레이스케일)의 28x28픽셀 이미지 데이터가 들어 있음을 나타낸다. \n",
    "- label에는 정답 숫자의 인덱스(0~9)가 들어 있다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 5\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# 데이터 예시\n",
    "\n",
    "x, t = train_set[0] # 0번째 (data, label) 추출\n",
    "plt.imshow(x.reshape(28,28), cmap=mpl.cm.binary) #넘파이 타입 데이터를  이미지로 출력\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print('label:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/51-2.png\" width=\"50%\" height=\"50%\"></img>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 손글씨 이미지 데이터를 지금부터 신경망으로 학습할 것이다. \n",
    "- 입력데이터를 전처리하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x = x.flatten()\n",
    "    x = x.astype(np.float32)\n",
    "    x /= 255.0\n",
    "    return x\n",
    "\n",
    "train_set = dezero.datasets.MNIST(train=True, transform=f)\n",
    "test_set = dezero.datasets.MNIST(train=False, transform=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (1,28,28) 형상인 입력 데이터를 1열로 나열(평탄화flatten)하여 (784,)형상으로 변환한다. \n",
    "- 데이터 타입은 np.float32(32비트 부동소수점)로 변환한다. \n",
    "- 마지막으로 255.0으로 나눠서 값의 범위가 0.0 ~ 1.0 사이가 되도록 한다. \n",
    "- MNIST 클래스에서는 방금 설명한 전처리가 기본으로 설정되어 있다. \n",
    "- dezero.datasets.MNIST(train=True)로 호출하면 이상의 전처리가 자동으로 수행된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 51.2 MNIST 데이터 학습하기\n",
    "- 학습해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "train loss: 1.9304, accuracy: 0.2800\n",
      "test loss: 1.3898, accuracy: 0.4367\n",
      "epoch: 2\n",
      "train loss: 1.3086, accuracy: 0.4367\n",
      "test loss: 1.2032, accuracy: 0.5100\n",
      "epoch: 3\n",
      "train loss: 1.1784, accuracy: 0.4767\n",
      "test loss: 1.1303, accuracy: 0.5033\n",
      "epoch: 4\n",
      "train loss: 1.1146, accuracy: 0.5333\n",
      "test loss: 1.0888, accuracy: 0.5867\n",
      "epoch: 5\n",
      "train loss: 1.0726, accuracy: 0.6300\n",
      "test loss: 1.0534, accuracy: 0.5433\n"
     ]
    }
   ],
   "source": [
    "from dezero import DataLoader\n",
    "\n",
    "max_epoch = 5\n",
    "batch_size = 100\n",
    "hidden_size = 1000\n",
    "\n",
    "train_set #= dezero.datasets.MNIST(train=True)\n",
    "test_set #= dezero.datasets.MNIST(train=False)\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "model = MLP((hidden_size, 10))\n",
    "optimizer = optimizers.SGD().setup(model)\n",
    "#model = MLP((hidden_size, hidden_size, 10), activation=F.relu)\n",
    "#optimizer = optimizers.Adam().setup(model)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "\n",
    "    for x, t in train_loader:\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        acc = F.accuracy(y, t)\n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('epoch: {}'.format(epoch+1))\n",
    "    print('train loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(train_set), sum_acc / len(train_set)))\n",
    "\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    with dezero.no_grad():\n",
    "        for x, t in test_loader:\n",
    "            y = model(x)\n",
    "            loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t)\n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('test loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(test_set), sum_acc / len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이번 단계에서 달라진 점은 MNIST 데이터셋을 사용한다는 것과 하이퍼파라미터값을 변경한 정도이다. \n",
    "- 인식 정확도는 테스트 데이터셋에서 약 86%를 얻었다. \n",
    "- 에포크 수를 늘리면 정확도가 올라가겠지만, 개선할 점이 눈에 띄어 마지막 정확한 모델로 개선해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 51.3 모델 개선\n",
    "- 방금 이용한 신경망의 활성화 함수는 시그모이드 함수였다. \n",
    "- 그런데 최근에는 ReLU (rectified linear unit)라는 함수가 더 자주 사용된다. \n",
    "- ReLU는 입력이 0보다 크면 입력 그대로 출력하고, 0 이하면 0을 출력하는 함수이다. \n",
    "$$h(x)=\\begin{cases}\n",
    "x \\text{ }(x>0) \\\\\n",
    "0 \\text{ }(x\\leq0)\n",
    "\\end{cases}$$\n",
    "- 아래와 같이 쉽게 구현했다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Function):\n",
    "    def forward(self, x):\n",
    "        xp = cuda.get_array_module(x)\n",
    "        y = xp.maximum(x, 0.0) # (1)\n",
    "        return y\n",
    "\n",
    "    def backward(self, gy):\n",
    "        x, = self.inputs\n",
    "        mask = x.data > 0 #(2)\n",
    "        gx = gy * mask #(3)\n",
    "        return gx\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return ReLU()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (1) 순전파에서는 np.maximum(x, 0, 0)에 의해 x의 원소와 0.0중 큰 쪽을 반환한다. \n",
    "- 역전파에서는 입력 x에서 0보다 큰 원소에 해당하는 위치의 기울기는 그대로 흘려보내고,\n",
    "- 0 이하라면 기울기를 0으로 설정해야한다. \n",
    "- 따라서 출력쪽에서 전해지는 기울기를 '통과시킬지'표시한 마스크(mask)를 준비한 후 (2), 기울기를 곱한다.(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = MLP((hidden_size, 10))\n",
    "model = MLP((hidden_size, hidden_size, 10), activation=F.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ReLU 함수를 사용하여 새로운 3층 신경망을 만들었다. \n",
    "- 앞 절보다 층수를 늘려 표현력을 높인 것이다. \n",
    "- 활성화 함수를 ReLU로 바꿔서 학습이 더 효율적으로 이루어질 것이다. \n",
    "- 신경망을 사용하여 최적화 기법을 SGD에서 Adam으로 바꾼 후 학습을 수행하면 훈련욜 데이터로는 99%, 테스트용 데이터로는 약 98%라는 인식 정확도를 얻을 것이다. \n",
    "\n",
    "******\n",
    "- 이것으로 4고지 정복했다. \n",
    "- 드디어 DeZero를 딥러닝 프레임워크라고  부를 수 있게 되었다. \n",
    "- 지금까지 배운지식은 파이토치와 체이너 같은 유명 프레임워크와 통한다. \n",
    "- 우리는 '살아있는 지식'을 손에 넣었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 칼럼 : 딥러닝 프레임워크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define-by-Run 방식의 자동미분\n",
    "- 계층 컬렉션\n",
    "- 옵티마이져 컬렉션\n",
    "\n",
    "\n",
    "`[DeZero 프레임워크 핵심 클래스 구조]`    \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/WegraLee/deep-learning-from-scratch-3/master/DeZeroClasses.png\"></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
